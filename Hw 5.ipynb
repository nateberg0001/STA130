{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc30b636",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "\n",
    "What is the key factor that makes the difference between ideas that can, and cannot be examined and tested statistically? \n",
    "The ability to measure or observe outcomes in an unbiased, repeatable way.\n",
    "\n",
    "What would you describe is the key \"criteria\" defining what a good null hypothesis is?\n",
    "The null hypothesis needs to be the nonspecial case of what ever you are analysizing.\n",
    "\n",
    "And what is the difference between a null hypothesis and an alternative hypothesis in the context of hypothesis testing? \n",
    "The null hypothesis needs to be the nonspecial case of what ever you are analysizing, whereas the alternative hypothesis is the outcome the statistician would like to get information about.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5f25b9",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "\n",
    "Statistical tests are done to get information about the population (called population parameters), like the population mean, denoted $\\mu$. The first step in this process is to gather a sample, which is a set of values, denoted $x_i$ such that $1\\le i\\le n$ where n is the number of data points. Then, we use our values to calculate statistics like the sample mean, denoted $\\bar{x}$. If we aim to setup up a hypothesis test, it is also important to consider null parameters like $\\mu_0$ (the theoretical mean if the null hypothesis is true), to ground our tests with some certainty (discussed in detail in Q3). After constructing these statisticals tests, we can run the calculations and use the results to draw conclusions about characteristics of the population."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54dce9f",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "\n",
    "Statistics tests are grounded in comparisons to a set of standardized distributions. Assuming the null hypothesis is true allows us to construct a specific model that we can use to perform statisticals tests. For example, the normal distribution requires a center parameter and the null hypothesis can provide this center very concretely. Or, if we are contructing a confidence interval, we need a concrete value, like 0, to see check if it is in the interval. In conclusion, without assuming the null hypothesis is true, there would be too much uncertainty to start our statisticals tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c98192",
   "metadata": {},
   "source": [
    "# Question 4\n",
    "\n",
    "The definition of a p-value is the probability of getting a specific sample statistic (value) given that the null hypothesis is true. As the p-value decreases, the probability decreases and it becomes more and more unlikely (more \"ridiculous\" if you will) that a sample would result in the sample statistic (value). Essentially, at low enough p-values, it becomes absurd to think that the data collected would be collected from a population where the null hypothesis is true."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6a3ba0",
   "metadata": {},
   "source": [
    "# Question 5\n",
    "\n",
    "We can use probability distribution function of a binomial distribution with n = 124 and p = .5 and then plug in our value of 80 to get a p-value. Here is the code (created myself):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f34a0970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The p-value is 0.0007823670130848726\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "\n",
    "p_value = 1 - scipy.stats.binom.cdf(79,124,.5)\n",
    "print(\"The p-value is \" + str(p_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec757618",
   "metadata": {},
   "source": [
    "The p-value is lower than all generally excepted alpha levels, so there is strong evidence against the null hypothesis. In other words, we have sufficient evidence to reject the null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06bdc8e",
   "metadata": {},
   "source": [
    "# Question 6\n",
    "\n",
    "P-values can only be used to reject the null hypothesis, never to confirm the alternate hypothesis. In this way, no matter how low the p-value is, we cannot prove Fido innocent because there is always a chance he is guilty. Likewise, no matter how high the p-value is, we cannot prove Fido guilty because there is always a chance he is innocent. P-values resulting from a statistical test will never be 0 or 1, but if it hypothetically did, it would be enough to determine the innocence of Fido."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f323e43c",
   "metadata": {},
   "source": [
    "# Question 7\n",
    "\n",
    "I took the code from Demo II of Tutorial 5 and realized it is rather simple to switch it to a one-sided test. It only the matter of getting rid of the absolute value signs. More specifically, all you have to do is change the criteria from the \n",
    "$ abs(sample statistics - population parameter) >= abs(observed statistic - population parameter) $ \n",
    "to \n",
    "$ sample statistics - population parameter >= observed statistic - population parameter $. \n",
    "\n",
    "This will change the counts of bootstrapped sample to only those more extreme in the positive direction, or in other words, a one-sided test. Our hypotheses change from $H_0 : S_f - S_I = 0, H_A : S_f - S_I \\neq 0$ to $N_0 : S_f - S_I = 0, N_A : S_f - S_I > 0$. Originally the p-value was approximately .068, but now it is .0565, so the p-value decreased. This makes sense, because we are restricting our criteria for the counts as mentioned above. \n",
    "\n",
    "This is the finalized code (edited by me without the help of CHATGPT):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18790bcd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which simulated statistics are \"as or more extreme\"\n",
      "than the observed statistic? (of 0.8)\n",
      "Number of Simulations: 10000\n",
      "\n",
      "Number of simulated statistics (under HO)\n",
      "that are \"as or more extreme\" than the observed statistic: 565\n",
      "\n",
      "p-value\n",
      "(= simulations \"as or more extreme\" / total simulations): 0.0565\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "patient_data = pd.DataFrame({\n",
    "    \"PatientID\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    \"Age\": [45, 34, 29, 52, 37, 41, 33, 48, 26, 39],\n",
    "    \"Gender\": [\"M\", \"F\", \"M\", \"F\", \"M\", \"F\", \"M\", \"F\", \"M\", \"F\"],\n",
    "    \"InitialHealthScore\": [84, 78, 83, 81, 81, 80, 79, 85, 76, 83],\n",
    "    \"FinalHealthScore\": [86, 86, 80, 86, 84, 86, 86, 82, 83, 84]\n",
    "})\n",
    "\n",
    "patient_data['HealthScoreChange'] = patient_data.FinalHealthScore-patient_data.InitialHealthScore\n",
    "\n",
    "#print(pd.DataFrame({'HealthScoreChange': patient_data['HealthScoreChange'],\n",
    "                    #'> 0 ?': patient_data['HealthScoreChange']>0}))\n",
    "\n",
    "random_difference_sign = np.random.choice([-1, 1], size=len(patient_data))\n",
    "pd.DataFrame({'HealthScoreChange': random_difference_sign*patient_data['HealthScoreChange'].abs(),\n",
    "              '> 0 ?': (random_difference_sign*patient_data['HealthScoreChange'])>0})\n",
    "# And then can you see what's happening here???\n",
    "\n",
    "np.random.seed(1)  # make simulation reproducible\n",
    "number_of_simulations = 10000  # experiment with this... what does this do?\n",
    "n_size = len(patient_data)  # 10\n",
    "IncreaseProportionSimulations_underH0random = np.zeros(number_of_simulations)\n",
    "\n",
    "# generate \"random improvement\" proportions assuming H0 (vaccine has no average effect) is true \n",
    "# meaning that the \"before and after\" differences are positive or negative at \"random\"\n",
    "for i in range(number_of_simulations):\n",
    "    \n",
    "    # why is this equivalent to the suggested idea above?\n",
    "    random_improvement = np.random.choice([0,1], size=len(patient_data), replace=True)  # <<< `replace=True` ^^^\n",
    "\n",
    "    # why is .mean() a proportion? \n",
    "    IncreaseProportionSimulations_underH0random[i] = random_improvement.mean()\n",
    "    # why is this the statistic we're interested in? Hint: next section...\n",
    "\n",
    "    population_parameter_value_under_H0 = 0.5\n",
    "\n",
    "observed_statistic = (patient_data.HealthScoreChange>0).mean()\n",
    "simulated_statistics = IncreaseProportionSimulations_underH0random\n",
    "\n",
    "SimStats_as_or_more_extreme_than_ObsStat = \\\n",
    "    simulated_statistics - population_parameter_value_under_H0 >= \\\n",
    "    observed_statistic - population_parameter_value_under_H0 \n",
    "    \n",
    "print('''Which simulated statistics are \"as or more extreme\"\n",
    "than the observed statistic? (of ''', observed_statistic, ')', sep=\"\")\n",
    "\n",
    "pd.DataFrame({'(Simulated) Statistic': simulated_statistics,\n",
    "              '>= '+str(observed_statistic)+\" ?\": ['>= '+str(observed_statistic)+\" ?\"]*number_of_simulations, \n",
    "              '\"as or more extreme\"?': SimStats_as_or_more_extreme_than_ObsStat})\n",
    "\n",
    "# Calculate the p-value\n",
    "# How many bootstrapped statistics generated under H0 \n",
    "# are \"as or more extreme\" than the observed statistic \n",
    "# (relative to the hypothesized population parameter)? \n",
    "\n",
    "observed_statistic = (patient_data.HealthScoreChange>0).mean()\n",
    "simulated_statistics = IncreaseProportionSimulations_underH0random\n",
    "\n",
    "# Be careful with \"as or more extreme\" as it's symmetric!\n",
    "SimStats_as_or_more_extreme_than_ObsStat = \\\n",
    "    simulated_statistics - population_parameter_value_under_H0 >= \\\n",
    "    observed_statistic - population_parameter_value_under_H0\n",
    "    \n",
    "p_value = (SimStats_as_or_more_extreme_than_ObsStat).sum() / number_of_simulations\n",
    "print(\"Number of Simulations: \", number_of_simulations, \"\\n\\n\",\n",
    "      \"Number of simulated statistics (under HO)\\n\",\n",
    "      'that are \"as or more extreme\" than the observed statistic: ',\n",
    "      SimStats_as_or_more_extreme_than_ObsStat.sum(), \"\\n\\n\",\n",
    "      'p-value\\n(= simulations \"as or more extreme\" / total simulations): ', p_value, sep=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbce333",
   "metadata": {},
   "source": [
    "# Question 8 \n",
    "\n",
    "Our experiment is similar to the Fisher's original, but we are targeting a different population and asking each participant only once. In this experiment, we are trying to draw conclusions about whether STA130 students (the population) can tell the difference between the teas. Unlike the original, we are interested in a group of people, not just one person, so sampling is recommended. In this case, we selected 80 students (presumably randomly) to act as our sample and recorded their answers.\n",
    "\n",
    "Statistic: the proportion of students sampled who correctly answer what came first (defined $\\hat{p}$).\n",
    "\n",
    "Parameter: the proportion of all STA130 students who would correctly answer what came first (defined p).\n",
    "\n",
    "$H_0 :$ students cannot tell the difference $(\\mu = p = .5)$\n",
    "\n",
    "$H_A :$ The null hypothesis is false $(\\mu \\neq .5)$\n",
    "\n",
    "Calculation setup:\n",
    "Because n is high enough, we can generalize to the normal model, with mean .5 from null hypothesis and standard deviation of $\\sqrt{\\frac{p*q}{n}} \\approx 0.056 $. Then, we will calculate a z-score, using $ (\\hat{p} - p) / S.D =  .1125 / 0.056 \\approx 2 $. Then we can use the cdf function of the normal distribution to calculate a p-value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96bced18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The p-value is 0.0441713449084425\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "\n",
    "#given variables\n",
    "p=.5\n",
    "q=1-p\n",
    "n=80\n",
    "p_hat=49/80\n",
    "#Standard Deviation Calculation\n",
    "s_d=((p*q)/n)**.5\n",
    "#Z score Calculation (we want the negative one)\n",
    "z_s=(p-p_hat)/s_d\n",
    "#Note because we are doing a two-sided test, we multiply by two\n",
    "p_value = (scipy.stats.norm.cdf(z_s))*2\n",
    "print(\"The p-value is \" + str(p_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a1b8f5",
   "metadata": {},
   "source": [
    "Because the p-value is between 0 and .05, we have moderate evidence against the null hypothesis. In other words, there is sufficient evidence to suggest that the students of STA130 can tell in which order the tea was mixed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56775194",
   "metadata": {},
   "source": [
    "# Question 9\n",
    "\n",
    "Yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc84b96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
